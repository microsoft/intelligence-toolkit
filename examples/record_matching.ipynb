{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload and format data\n",
    "# 2 datasets\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import polars as pl\n",
    "\n",
    "from toolkit.record_matching.prepare_model import (\n",
    "    build_attribute_options,\n",
    "    format_dataset,\n",
    ")\n",
    "\n",
    "# Load data\n",
    "df1 = pl.read_csv(\"file1.csv\")\n",
    "df2 = pl.read_csv(\"file2.csv\")\n",
    "\n",
    "# add dataset already with name, entity name, maybe id and attr columns\n",
    "\n",
    "matching_dfs = {}\n",
    "matching_dfs[\"file1\"] = format_dataset(\n",
    "    df1, [\" JoinDate\", \" Salary  \", \"EmployeeID\"], \" FullName\"\n",
    ")\n",
    "matching_dfs[\"file2\"] = format_dataset(\n",
    "    df2, [\" Start_Date\", \" Budget  \", \"ProjID\"], \" TeamLead\"\n",
    ")\n",
    "\n",
    "attr_options = build_attribute_options(matching_dfs)\n",
    "attr_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit.record_matching.prepare_model import build_attribute_list\n",
    "from toolkit.record_matching.config import AttributeToMatch\n",
    "\n",
    "\n",
    "attributes = [\n",
    "    AttributeToMatch(\n",
    "        {\n",
    "            \"label\": \" JoinDate\",\n",
    "            \"columns\": [\" JoinDate::file1\", \" Start_Date::file2\"],\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "atts_to_datasets = build_attribute_list(attributes)\n",
    "atts_to_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit.record_matching.detect import build_attributes_dataframe\n",
    "\n",
    "\n",
    "merged_df = build_attributes_dataframe(\n",
    "    matching_dfs,\n",
    "    atts_to_datasets,\n",
    ")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.with_columns(\n",
    "    ((pl.col(\"Entity ID\").cast(pl.Utf8)) + \"::\" + pl.col(\"Dataset\")).alias(\"Unique ID\")\n",
    ")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from toolkit.AI.openai_embedder import OpenAIEmbedder\n",
    "from toolkit.AI.openai_configuration import OpenAIConfiguration\n",
    "from toolkit.record_matching.detect import convert_to_sentences\n",
    "\n",
    "ai_configuration = OpenAIConfiguration(\n",
    "    {\n",
    "        \"api_type\": \"OpenAI\",\n",
    "        \"api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "        \"model\": \"gpt-4o-2024-08-06\",\n",
    "    }\n",
    ")\n",
    "\n",
    "text_embedder = OpenAIEmbedder(\n",
    "    configuration=ai_configuration,\n",
    ")\n",
    "\n",
    "all_sentences = convert_to_sentences(merged_df)\n",
    "embeddings = text_embedder.embed_store_many(all_sentences)\n",
    "print(len(embeddings), \" embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit.record_matching.detect import (\n",
    "    build_near_map,\n",
    "    build_nearest_neighbors,\n",
    "    build_sentence_pair_scores,\n",
    ")\n",
    "\n",
    "\n",
    "distances, indices = build_nearest_neighbors(embeddings)\n",
    "\n",
    "sentence_pair_embedding_threshold = 0.05\n",
    "near_map = build_near_map(\n",
    "    distances,\n",
    "    indices,\n",
    "    all_sentences,\n",
    "    sentence_pair_embedding_threshold,\n",
    ")\n",
    "\n",
    "sentence_pair_scores = build_sentence_pair_scores(near_map, merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit.record_matching.detect import build_matches, build_matches_dataset\n",
    "\n",
    "matching_sentence_pair_jaccard_threshold = 0.5\n",
    "entity_to_group, matches, pair_to_match = build_matches(\n",
    "    sentence_pair_scores, merged_df, matching_sentence_pair_jaccard_threshold\n",
    ")\n",
    "\n",
    "matches_df_final = pl.DataFrame(\n",
    "    list(matches),\n",
    "    schema=[\"Group ID\", *merged_df.columns],\n",
    ").sort(by=[\"Group ID\", \"Entity name\", \"Dataset\"], descending=False)\n",
    "\n",
    "matches_df_final = build_matches_dataset(\n",
    "    matches_df_final, pair_to_match, entity_to_group\n",
    ")\n",
    "\n",
    "print(matches_df_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
